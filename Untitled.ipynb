{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from Model import actor, get_network_input\n",
    "from Carrot import GameEnvironment\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = []\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        done_batch = []\n",
    "        \n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        for experience in batch:\n",
    "            state, action, reward, next_state, done = experience\n",
    "            state_batch.append(state)\n",
    "            action_batch.append(action)\n",
    "            reward_batch.append(reward)\n",
    "            next_state_batch.append(next_state)\n",
    "            done_batch.append(done)\n",
    "        \n",
    "        return (state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
    "    \n",
    "    def truncate(self):\n",
    "        self.buffer = self.buffer[-self.max_size:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = actor(2, 10, 2)\n",
    "board = GameEnvironment()\n",
    "memory = ReplayMemory(100)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "gamma = 0.45\n",
    "\n",
    "def play(num_games, printing=False):\n",
    "    run = True\n",
    "    move=0\n",
    "    games_played = 0\n",
    "    while run:\n",
    "        inp = torch.from_numpy(board.get_boardstate())\n",
    "        out = model(inp)\n",
    "        rand = np.random.uniform(0,1)\n",
    "        if rand > epsilon:\n",
    "            move = torch.argmax(out)\n",
    "        else:\n",
    "            move = np.random.randint(0,2)\n",
    "        if printing == True:\n",
    "            print(inp, out)\n",
    "\n",
    "        reward, done = board.update_boardstate(move)\n",
    "        #print(inp, out, reward)\n",
    "        newinp = torch.from_numpy(board.get_boardstate())\n",
    "        \n",
    "        memory.push(inp, move, reward, newinp, done)\n",
    "        games_played += 1\n",
    "        \n",
    "        if num_games == games_played:\n",
    "            run = False\n",
    "            \n",
    "MSE = nn.MSELoss()\n",
    "def lossf(batch):\n",
    "    states, actions, rewards, next_states, dones = batch\n",
    "    states = torch.cat([x.unsqueeze(0) for x in states], dim=0).to(device)\n",
    "    actions = torch.LongTensor(actions).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).to(device)\n",
    "    next_states = torch.cat([x.unsqueeze(0) for x in next_states]).to(device)\n",
    "    dones = torch.FloatTensor(dones).to(device)\n",
    "    \n",
    "    curr_Q = model.forward(states)\n",
    "    next_Q = model.forward(next_states)\n",
    "    \n",
    "    curr_Qy = curr_Q.gather(1, actions.unsqueeze(0).transpose(0,1)).transpose(0,1).squeeze(0)\n",
    "    max_next_Q = torch.max(next_Q, 1)[0]\n",
    "    expected_Q = rewards + gamma*max_next_Q\n",
    "    loss = MSE(curr_Qy, expected_Q)\n",
    "    \n",
    "    '''\n",
    "    print('STATE', states)\n",
    "    print('ACTIONS', actions)\n",
    "    print('REWARDS', rewards)\n",
    "    print('NEXT STATES', next_states)\n",
    "    print('DONES', dones)\n",
    "    print('CURR_Q', curr_Q)\n",
    "    print('CURR_Q AFTER ACTIONS', curr_Qy)\n",
    "    print('NEXT_Q', next_Q)\n",
    "    print('EXPECTED Q', expected_Q)\n",
    "    print('LOSS', loss)\n",
    "    #'''\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def learn(times, batch_size):\n",
    "    total_loss = 0\n",
    "    for i in range(times):\n",
    "        optimizer.zero_grad()\n",
    "        sample = memory.sample(batch_size)\n",
    "        loss = lossf(sample)\n",
    "        total_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('LOSS {}'.format(total_loss))\n",
    "    \n",
    "def train_bot(games_per_epoch, batch_size):\n",
    "    while True:\n",
    "        play(games_per_epoch)\n",
    "        learn(100, batch_size)\n",
    "        memory.truncate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS 1000005.5\n",
      "LOSS 999381.125\n",
      "LOSS 998806.75\n",
      "LOSS 998668.3125\n",
      "LOSS 998486.125\n",
      "LOSS 998356.5\n",
      "LOSS 998444.375\n",
      "LOSS 998219.625\n",
      "LOSS 998034.375\n",
      "LOSS 997824.0625\n",
      "LOSS 997674.875\n",
      "LOSS 997495.6875\n",
      "LOSS 997436.625\n",
      "LOSS 997223.1875\n",
      "LOSS 997048.1875\n",
      "LOSS 996804.8125\n",
      "LOSS 996595.5\n",
      "LOSS 996385.6875\n",
      "LOSS 996135.5625\n",
      "LOSS 996025.0625\n",
      "LOSS 995532.25\n",
      "LOSS 995229.8125\n",
      "LOSS 994770.125\n",
      "LOSS 994300.5\n",
      "LOSS 994052.6875\n",
      "LOSS 993520.0\n",
      "LOSS 992839.625\n",
      "LOSS 992189.125\n",
      "LOSS 991803.0625\n",
      "LOSS 990952.875\n",
      "LOSS 990254.6875\n",
      "LOSS 989480.75\n",
      "LOSS 988549.4375\n",
      "LOSS 987201.3125\n",
      "LOSS 986078.6875\n",
      "LOSS 984996.8125\n",
      "LOSS 984033.3125\n",
      "LOSS 981917.75\n",
      "LOSS 979698.125\n",
      "LOSS 979235.8125\n",
      "LOSS 977270.25\n",
      "LOSS 973899.4375\n",
      "LOSS 972735.0625\n",
      "LOSS 969844.5625\n",
      "LOSS 967786.0\n",
      "LOSS 964043.75\n",
      "LOSS 958649.5\n",
      "LOSS 956674.3125\n",
      "LOSS 958382.5625\n",
      "LOSS 952482.5\n",
      "LOSS 950182.0625\n",
      "LOSS 945973.0\n",
      "LOSS 948079.8125\n",
      "LOSS 945931.375\n",
      "LOSS 939537.75\n",
      "LOSS 928968.25\n",
      "LOSS 931281.25\n",
      "LOSS 931081.6875\n",
      "LOSS 909996.25\n",
      "LOSS 907249.125\n",
      "LOSS 897319.5625\n",
      "LOSS 902729.1875\n",
      "LOSS 898705.6875\n",
      "LOSS 894771.875\n",
      "LOSS 877001.3125\n",
      "LOSS 867133.4375\n",
      "LOSS 865476.0\n",
      "LOSS 850406.5625\n",
      "LOSS 848734.0\n",
      "LOSS 847450.625\n",
      "LOSS 827096.5625\n",
      "LOSS 843725.1875\n",
      "LOSS 822065.1875\n",
      "LOSS 806322.875\n",
      "LOSS 804367.6875\n",
      "LOSS 792482.4375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5bbb93eba4ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_bot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-3abecde05378>\u001b[0m in \u001b[0;36mtrain_bot\u001b[1;34m(games_per_epoch, batch_size)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgames_per_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-3abecde05378>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(times, batch_size)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LOSS {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_bot(100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0], dtype=torch.int32) tensor([27.4201,  7.4293], grad_fn=<AddBackward0>)\n",
      "tensor([1, 0], dtype=torch.int32) tensor([27.4201,  7.4293], grad_fn=<AddBackward0>)\n",
      "tensor([1, 0], dtype=torch.int32) tensor([27.4201,  7.4293], grad_fn=<AddBackward0>)\n",
      "tensor([0, 1], dtype=torch.int32) tensor([11.8043, 17.9294], grad_fn=<AddBackward0>)\n",
      "tensor([0, 1], dtype=torch.int32) tensor([11.8043, 17.9294], grad_fn=<AddBackward0>)\n",
      "tensor([1, 0], dtype=torch.int32) tensor([27.4201,  7.4293], grad_fn=<AddBackward0>)\n",
      "tensor([0, 1], dtype=torch.int32) tensor([11.8043, 17.9294], grad_fn=<AddBackward0>)\n",
      "tensor([1, 0], dtype=torch.int32) tensor([27.4201,  7.4293], grad_fn=<AddBackward0>)\n",
      "tensor([1, 0], dtype=torch.int32) tensor([27.4201,  7.4293], grad_fn=<AddBackward0>)\n",
      "tensor([1, 0], dtype=torch.int32) tensor([27.4201,  7.4293], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "play(10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
